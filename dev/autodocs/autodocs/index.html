<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Automatic Documentations · Photon.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://MKAbdElrahman.github.io/Photon.jl/autodocs/autodocs/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Photon.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Automatic Documentations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Automatic Documentations</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/MKAbdElrahman/Photon.jl/blob/master/docs/src/autodocs/autodocs.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Automatic-Documentations"><a class="docs-heading-anchor" href="#Automatic-Documentations">Automatic Documentations</a><a id="Automatic-Documentations-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Documentations" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Photon.ADADelta" href="#Photon.ADADelta"><code>Photon.ADADelta</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ADADelta(ρ = 0.9)</code></pre><p><a href="https://arxiv.org/abs/1212.5701">ADADelta</a> is a version of ADAGrad adapting its learning rate based on a window of past gradient updates. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Rho (<code>ρ</code>): Factor by which the gradient is decayed at each time step.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = ADADelta()
opt = ADADelta(0.89)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L304-L316">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.ADAGrad" href="#Photon.ADAGrad"><code>Photon.ADAGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ADAGrad(η = 0.1)</code></pre><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">ADAGrad</a> optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = ADAGrad()
opt = ADAGrad(0.001)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L276-L289">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.ADAM" href="#Photon.ADAM"><code>Photon.ADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ADAM(η = 0.001, β::Tuple = (0.9, 0.999))</code></pre><p><a href="https://arxiv.org/abs/1412.6980">ADAM</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = ADAM()
opt = ADAM(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L117-L130">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.AMSGrad" href="#Photon.AMSGrad"><code>Photon.AMSGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AMSGrad(η = 0.001, β::Tuple = (0.9, 0.999))</code></pre><p>The <a href="https://openreview.net/forum?id=ryQu7f-RZ">AMSGrad</a> version of the ADAM optimiser. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AMSGrad()
opt = AMSGrad(0.001, (0.89, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L335-L349">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.AdaBelief" href="#Photon.AdaBelief"><code>Photon.AdaBelief</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AdaBelief(η = 0.001, β::Tuple = (0.9, 0.999))</code></pre><p>The <a href="https://arxiv.org/abs/2010.07468">AdaBelief</a> optimiser is a variant of the well-known ADAM optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AdaBelief()
opt = AdaBelief(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L429-L443">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.AdaMax" href="#Photon.AdaMax"><code>Photon.AdaMax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AdaMax(η = 0.001, β::Tuple = (0.9, 0.999))</code></pre><p><a href="https://arxiv.org/abs/1412.6980">AdaMax</a> is a variant of ADAM based on the ∞-norm.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AdaMax()
opt = AdaMax(0.001, (0.9, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L199-L212">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.Descent" href="#Photon.Descent"><code>Photon.Descent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Descent(η = 0.1)</code></pre><p>Classic gradient descent optimiser with learning rate <code>η</code>. For each parameter <code>p</code> and its gradient <code>δp</code>, this runs <code>p -= η*δp</code></p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L8-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.Momentum" href="#Photon.Momentum"><code>Photon.Momentum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Momentum(η = 0.01, ρ = 0.9)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = Momentum()
opt = Momentum(0.01, 0.99)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L26-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.NADAM" href="#Photon.NADAM"><code>Photon.NADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NADAM(η = 0.001, β::Tuple = (0.9, 0.999))</code></pre><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">NADAM</a> is a Nesterov variant of ADAM. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = NADAM()
opt = NADAM(0.002, (0.89, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L371-L385">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.Nesterov" href="#Photon.Nesterov"><code>Photon.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Nesterov(η = 0.001, ρ = 0.9)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and Nesterov momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Nesterov momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = Nesterov()
opt = Nesterov(0.003, 0.95)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L55-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.OADAM" href="#Photon.OADAM"><code>Photon.OADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">OADAM(η = 0.0001, β::Tuple = (0.5, 0.9))</code></pre><p><a href="https://arxiv.org/abs/1711.00141">OADAM</a> (Optimistic ADAM) is a variant of ADAM adding an &quot;optimistic&quot; term suitable for adversarial training.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = OADAM()
opt = OADAM(0.001, (0.9, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L236-L250">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.RADAM" href="#Photon.RADAM"><code>Photon.RADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RADAM(η = 0.001, β::Tuple = (0.9, 0.999))</code></pre><p><a href="https://arxiv.org/abs/1908.03265">Rectified ADAM</a> optimizer.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = RADAM()
opt = RADAM(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L154-L167">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.RMSProp" href="#Photon.RMSProp"><code>Photon.RMSProp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RMSProp(η = 0.001, ρ = 0.9)</code></pre><p>Optimizer using the <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = RMSProp()
opt = RMSProp(0.002, 0.95)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L85-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.ADAMW" href="#Photon.ADAMW"><code>Photon.ADAMW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ADAMW(η = 0.001, β::Tuple = (0.9, 0.999), decay = 0)</code></pre><p><a href="https://arxiv.org/abs/1711.05101">ADAMW</a> is a variant of ADAM fixing (as in repairing) its weight decay regularization.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li><code>decay</code>: Decay applied to weights during optimisation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = ADAMW()
opt = ADAMW(0.001, (0.89, 0.995), 0.1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Optimization/Optimizers.jl#L410-L425">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.Coordinate-Tuple{Grid, Photon.GridType{:Dual}, CartesianIndex}" href="#Photon.Coordinate-Tuple{Grid, Photon.GridType{:Dual}, CartesianIndex}"><code>Photon.Coordinate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Coordinate(g::Grid ,::GridType{:Dual},Ind::CartesianIndex)</code></pre><p>Returns the equivalent dual coordinate of of a <code>CartesianIndex</code>.</p><p>The function doesnot chack for being inside the grid.	</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Interface/Grid.jl#L69-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.Coordinate-Tuple{Grid, Photon.GridType{:Primal}, CartesianIndex}" href="#Photon.Coordinate-Tuple{Grid, Photon.GridType{:Primal}, CartesianIndex}"><code>Photon.Coordinate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Coordinate(g::Grid , ::GridType{:Primal}, Ind::CartesianIndex)</code></pre><p>Returns the equivalent primal coordinate of of a <code>CartesianIndex</code>.</p><p>The function doesnot chack for being inside the grid.	</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Interface/Grid.jl#L80-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.Coordinates-Union{Tuple{T}, Tuple{Grid, Photon.GridType{T}, CartesianIndices}} where T" href="#Photon.Coordinates-Union{Tuple{T}, Tuple{Grid, Photon.GridType{T}, CartesianIndices}} where T"><code>Photon.Coordinates</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia"> Coordinates(g::Grid,gridtype::GridType{T},CIns::CartesianIndices ) where T</code></pre><p>Returns the primal or dual coordinates of a set of cartesian indecies.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Interface/Grid.jl#L59-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Photon.Coordinates-Union{Tuple{T}, Tuple{Grid, Photon.GridType{T}}} where T" href="#Photon.Coordinates-Union{Tuple{T}, Tuple{Grid, Photon.GridType{T}}} where T"><code>Photon.Coordinates</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">  Coordinates(g::Grid,gridtype::GridType{T}) where T</code></pre><p>Returns the primal or dual coordinates of all points on a grid.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MKAbdElrahman/Photon.jl/blob/a5084ed805ac58ea3851a3d8ab92a85a3687e17b/src/Interface/Grid.jl#L50-L54">source</a></section></article></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 10 July 2021 06:10">Saturday 10 July 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
