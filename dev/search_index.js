var documenterSearchIndex = {"docs":
[{"location":"autodocs/autodocs/#Automatic-Documentations","page":"Automatic Documentations","title":"Automatic Documentations","text":"","category":"section"},{"location":"autodocs/autodocs/","page":"Automatic Documentations","title":"Automatic Documentations","text":"Modules = [Photon]","category":"page"},{"location":"autodocs/autodocs/#Base.IteratorsMD.CartesianIndex-Union{Tuple{ND}, Tuple{Grid{ND}, Photon.GridType{:Dual}, Tuple{Vararg{Number, ND}}}} where ND","page":"Automatic Documentations","title":"Base.IteratorsMD.CartesianIndex","text":"CartesianIndex(g::Grid{ND},::GridType{:Dual},  coordinate::NTuple{ND,Number}) where ND\n\nReturns the equivalent dualCartesianIndex of a coordinate.\n\n\n\n\n\n","category":"method"},{"location":"autodocs/autodocs/#Photon.ADADelta","page":"Automatic Documentations","title":"Photon.ADADelta","text":"ADADelta(ρ = 0.9)\n\nADADelta is a version of ADAGrad adapting its learning rate based on a window of past gradient updates. Parameters don't need tuning.\n\nParameters\n\nRho (ρ): Factor by which the gradient is decayed at each time step.\n\nExamples\n\nopt = ADADelta()\nopt = ADADelta(0.89)\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.ADAGrad","page":"Automatic Documentations","title":"Photon.ADAGrad","text":"ADAGrad(η = 0.1)\n\nADAGrad optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\n\nExamples\n\nopt = ADAGrad()\nopt = ADAGrad(0.001)\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.ADAM","page":"Automatic Documentations","title":"Photon.ADAM","text":"ADAM(η = 0.001, β::Tuple = (0.9, 0.999))\n\nADAM optimiser.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n\nExamples\n\nopt = ADAM()\nopt = ADAM(0.001, (0.9, 0.8))\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.AMSGrad","page":"Automatic Documentations","title":"Photon.AMSGrad","text":"AMSGrad(η = 0.001, β::Tuple = (0.9, 0.999))\n\nThe AMSGrad version of the ADAM optimiser. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n\nExamples\n\nopt = AMSGrad()\nopt = AMSGrad(0.001, (0.89, 0.995))\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.AdaBelief","page":"Automatic Documentations","title":"Photon.AdaBelief","text":"AdaBelief(η = 0.001, β::Tuple = (0.9, 0.999))\n\nThe AdaBelief optimiser is a variant of the well-known ADAM optimiser.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n\nExamples\n\nopt = AdaBelief()\nopt = AdaBelief(0.001, (0.9, 0.8))\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.AdaMax","page":"Automatic Documentations","title":"Photon.AdaMax","text":"AdaMax(η = 0.001, β::Tuple = (0.9, 0.999))\n\nAdaMax is a variant of ADAM based on the ∞-norm.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n\nExamples\n\nopt = AdaMax()\nopt = AdaMax(0.001, (0.9, 0.995))\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.Descent","page":"Automatic Documentations","title":"Photon.Descent","text":"Descent(η = 0.1)\n\nClassic gradient descent optimiser with learning rate η. For each parameter p and its gradient δp, this runs p -= η*δp\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.Momentum","page":"Automatic Documentations","title":"Photon.Momentum","text":"Momentum(η = 0.01, ρ = 0.9)\n\nGradient descent optimizer with learning rate η and momentum ρ.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nMomentum (ρ): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.\n\nExamples\n\nopt = Momentum()\nopt = Momentum(0.01, 0.99)\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.NADAM","page":"Automatic Documentations","title":"Photon.NADAM","text":"NADAM(η = 0.001, β::Tuple = (0.9, 0.999))\n\nNADAM is a Nesterov variant of ADAM. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n\nExamples\n\nopt = NADAM()\nopt = NADAM(0.002, (0.89, 0.995))\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.Nesterov","page":"Automatic Documentations","title":"Photon.Nesterov","text":"Nesterov(η = 0.001, ρ = 0.9)\n\nGradient descent optimizer with learning rate η and Nesterov momentum ρ.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nNesterov momentum (ρ): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.\n\nExamples\n\nopt = Nesterov()\nopt = Nesterov(0.003, 0.95)\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.OADAM","page":"Automatic Documentations","title":"Photon.OADAM","text":"OADAM(η = 0.0001, β::Tuple = (0.5, 0.9))\n\nOADAM (Optimistic ADAM) is a variant of ADAM adding an \"optimistic\" term suitable for adversarial training.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n\nExamples\n\nopt = OADAM()\nopt = OADAM(0.001, (0.9, 0.995))\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.RADAM","page":"Automatic Documentations","title":"Photon.RADAM","text":"RADAM(η = 0.001, β::Tuple = (0.9, 0.999))\n\nRectified ADAM optimizer.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n\nExamples\n\nopt = RADAM()\nopt = RADAM(0.001, (0.9, 0.8))\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.RMSProp","page":"Automatic Documentations","title":"Photon.RMSProp","text":"RMSProp(η = 0.001, ρ = 0.9)\n\nOptimizer using the RMSProp algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nMomentum (ρ): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.\n\nExamples\n\nopt = RMSProp()\nopt = RMSProp(0.002, 0.95)\n\n\n\n\n\n","category":"type"},{"location":"autodocs/autodocs/#Photon.ADAMW","page":"Automatic Documentations","title":"Photon.ADAMW","text":"ADAMW(η = 0.001, β::Tuple = (0.9, 0.999), decay = 0)\n\nADAMW is a variant of ADAM fixing (as in repairing) its weight decay regularization.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\ndecay: Decay applied to weights during optimisation.\n\nExamples\n\nopt = ADAMW()\nopt = ADAMW(0.001, (0.89, 0.995), 0.1)\n\n\n\n\n\n","category":"function"},{"location":"autodocs/autodocs/#Photon.Coordinate-Tuple{Grid, Photon.GridType{:Dual}, CartesianIndex}","page":"Automatic Documentations","title":"Photon.Coordinate","text":"Coordinate(g::Grid ,::GridType{:Dual},Ind::CartesianIndex)\n\nReturns the equivalent dual coordinate of of a CartesianIndex.\n\nThe function doesnot chack for being inside the grid.\t\n\n\n\n\n\n","category":"method"},{"location":"autodocs/autodocs/#Photon.Coordinate-Tuple{Grid, Photon.GridType{:Primal}, CartesianIndex}","page":"Automatic Documentations","title":"Photon.Coordinate","text":"Coordinate(g::Grid , ::GridType{:Primal}, Ind::CartesianIndex)\n\nReturns the equivalent primal coordinate of of a CartesianIndex.\n\nThe function doesnot chack for being inside the grid.\t\n\n\n\n\n\n","category":"method"},{"location":"autodocs/autodocs/#Photon.Coordinates-Union{Tuple{T}, Tuple{Grid, Photon.GridType{T}, CartesianIndices}} where T","page":"Automatic Documentations","title":"Photon.Coordinates","text":" Coordinates(g::Grid,gridtype::GridType{T},CIns::CartesianIndices ) where T\n\nReturns the primal or dual coordinates of a set of cartesian indecies.\n\n\n\n\n\n","category":"method"},{"location":"autodocs/autodocs/#Photon.Coordinates-Union{Tuple{T}, Tuple{Grid, Photon.GridType{T}}} where T","page":"Automatic Documentations","title":"Photon.Coordinates","text":"  Coordinates(g::Grid,gridtype::GridType{T}) where T\n\nReturns the primal or dual coordinates of all points on a grid.\n\n\n\n\n\n","category":"method"},{"location":"basics/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"tutorials/example_1/#Scattering-From-A-2D-Disk","page":"Scattering From A 2D-Disk","title":"Scattering From A 2D-Disk","text":"","category":"section"},{"location":"basics/grid/#Defining-Grids","page":"Defining Grids","title":"Defining Grids","text":"","category":"section"},{"location":"basics/grid/","page":"Defining Grids","title":"Defining Grids","text":"In order to initialize a simulation, you have to define a grid upon which all computations will be done. Currently, Cartesian grids, with uniform spacing along each dimension are supported.","category":"page"},{"location":"basics/grid/","page":"Defining Grids","title":"Defining Grids","text":"Grids are defined by using the Grid constructor which must take as input two keword arguments:","category":"page"},{"location":"basics/grid/","page":"Defining Grids","title":"Defining Grids","text":"spacing = (dx,dy,..): the cell sizes for  each dimension. \nextent = (Lx,Ly,..):  the grid length for each dimension.","category":"page"},{"location":"basics/grid/","page":"Defining Grids","title":"Defining Grids","text":"spacing and extent mush have the same length. ","category":"page"},{"location":"basics/grid/#Examples:","page":"Defining Grids","title":"Examples:","text":"","category":"section"},{"location":"basics/grid/#D-Grid","page":"Defining Grids","title":"1D Grid","text":"","category":"section"},{"location":"basics/grid/","page":"Defining Grids","title":"Defining Grids","text":"using Photon              # hide\nLx = 15 \ndx = 0.05\nGrid(extent = (Lx,) , spacing = (dx,) )","category":"page"},{"location":"basics/grid/#D-Grid-2","page":"Defining Grids","title":"2D Grid","text":"","category":"section"},{"location":"basics/grid/","page":"Defining Grids","title":"Defining Grids","text":"using Photon              # hide\nLx = 15\nLy = 7.5 \ndx = 0.05\ndy = 0.2\nGrid(extent = (Lx,Ly) , spacing = (dx,dy) )","category":"page"},{"location":"basics/grid/#D-Grid-3","page":"Defining Grids","title":"3D Grid","text":"","category":"section"},{"location":"basics/grid/","page":"Defining Grids","title":"Defining Grids","text":"using Photon              # hide\nLx = 15\nLy = 7.5\nLz = 4.5 \ndx = 0.05\ndy = 0.2\ndz = 0.1\nGrid(extent = (Lx,Ly,Lz) , spacing = (dx,dy,dz) )","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Photon","category":"page"},{"location":"#Photon","page":"Home","title":"Photon","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Photon.","category":"page"},{"location":"#Getting-Started:-Installation-And-First-Steps","page":"Home","title":"Getting Started: Installation And First Steps","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the package, use the following command inside the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"https://github.com/MKAbdElrahman/Photon.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"To load the package, use the command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Photon","category":"page"},{"location":"#Tutorials","page":"Home","title":"Tutorials","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following tutorials will introduce you to the functionality of Photon.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"tutorials/example_1.md\"\n    ]\nDepth = 2","category":"page"},{"location":"#Basics","page":"Home","title":"Basics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"These pages introduce you to the core of Photon.jl and the common interface. It explains the general workflow, options which are generally available, and the general tools for analysis.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"basics/overview.md\",\n    \"basics/grid.md\",\n    ]\nDepth = 2","category":"page"},{"location":"#AutoDocs","page":"Home","title":"AutoDocs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"autodocs/autodocs.md\"\n    ]\nDepth = 2","category":"page"}]
}
